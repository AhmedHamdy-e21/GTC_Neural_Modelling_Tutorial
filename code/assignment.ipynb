{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from write_load import load_env\n",
    "from agent import DynaAgent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dyna agent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first part of the assignment is to reproduce some of the results from the original Dyna paper (Sutton, 1990). The pdf of the paper is somewhere in the git repository you have cloned. In particular, **your task is to generate and visualise data plotted in Figure 5** in that paper. You can neglect Dyna-PI and only implement Dyna-$Q-$ and Dyna-$Q+$.\n",
    "\n",
    "To make your life a little easier, and to let you jump right into the more interesting stuff, you are provided with the environment simulator located in `environment.py`, as well as a blueprint of the main code for the agent. That is, you have access to the file `agent.py` where you will find the agent class. This class has a method called `simulate` with the main simulation loop already implemented. You are of course invited to investigate it.\n",
    "\n",
    "**Your task is to fill in the missing implementation**. Thus, you are tasked to complete the following functions:\n",
    "- `_policy`. This is the typical $\\pi(a\\mid s)$ which specifies how the agent chooses actions in any given state\n",
    "- `_update_qvals`. This is the $Q$-learning update rule\n",
    "- `_update_experience_buffer`. This updates the agent's experience buffer from which it then samples planning updates\n",
    "- `_update_action_count`. This counts the number of moves elapsed since each action has last been attempted\n",
    "- `_plan`. This is the function which lets the agent plan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load environments\n",
    "maze_conf_path = os.path.abspath(os.path.join(os.getcwd(), '..', 'envs'))\n",
    "maze1_conf = load_env(os.path.join(maze_conf_path, 'dyna1.txt')) # maze with the path on the right open \n",
    "maze2_conf = load_env(os.path.join(maze_conf_path, 'dyna2.txt')) # maze with the path on the left open"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# these are the same as in the paper\n",
    "beta    = 0.5   # learning rate\n",
    "gamma   = 0.9   # discount factor\n",
    "epsilon = 0.001 # exploration parameter\n",
    "k       = 10    # number of planning updates\n",
    "\n",
    "# initialise the agent\n",
    "agent   = DynaAgent(beta, gamma, epsilon)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run simulations\n",
    "agent.init_env(**maze1_conf)\n",
    "agent.simulate(num_trials=1000, reset_agent=True, num_planning_updates=k)\n",
    "agent.init_env(**maze2_conf)\n",
    "agent.simulate(num_trials=2000, reset_agent=False, num_planning_updates=k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compare performance\n",
    "# plot figure 5 from Sutton (1990)\n",
    "perf = agent.get_performace()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, you will also implement what is known as the prioritized sweeping from Moore & Atkeson (1993). The pdf of the paper is also in this git repository.\n",
    "\n",
    "Even though the Dyna-style planning you have already implented confers significant learning advantages over comparable agents which do not plan, there is still room for improvement. In particular, it seems somewhat wasteful to adopt a uniform sampling scheme for the selection of planning updates. This is the key idea behind prioritized sweeping."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gtc",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
